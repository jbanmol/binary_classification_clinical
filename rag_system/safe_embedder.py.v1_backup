#!/usr/bin/env python3
"""
Safe Embedding Generator for RAG System
Fixes hanging issues in embedding generation with:
- Single-threaded processing
- Small batch processing  
- Robust error handling
- Memory management
- macOS compatibility
"""

import os
import time
import gc
import logging
import multiprocessing as mp
from typing import List, Optional, Iterable
import numpy as np
import warnings

# Set stability environment BEFORE any imports
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


class SafeEmbedder:
    """Safe, batched embedding generation with robust error handling"""
    
    def __init__(self, 
                 model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "cpu",
                 batch_size: int = 16,
                 max_retries: int = 3):
        """
        Initialize SafeEmbedder
        
        Args:
            model_name: Sentence transformer model name
            device: Device to use ('cpu' recommended for stability)
            batch_size: Number of texts to process at once
            max_retries: Number of retries for failed batches
        """
        self.model_name = model_name
        self.device = device
        self.batch_size = max(1, batch_size)
        self.max_retries = max_retries
        self._model = None
        
        # Clean up any existing multiprocessing children
        self._cleanup_multiprocessing()
    
    def _cleanup_multiprocessing(self):
        """Clean up any lingering multiprocessing children"""
        try:
            for child in mp.active_children():
                child.terminate()
                child.join(timeout=1.0)
        except Exception:
            pass
    
    def _ensure_model_loaded(self):
        """Lazy load the sentence transformer model"""
        if self._model is None:
            try:
                from sentence_transformers import SentenceTransformer
                logger.info(f"Loading embedding model {self.model_name} on {self.device}")
                self._model = SentenceTransformer(self.model_name, device=self.device)
                logger.info("Embedding model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                raise
    
    def encode_batch_safe(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        Safely encode a batch of texts with retries and error handling
        
        Args:
            texts: List of text strings to encode
            
        Returns:
            Numpy array of embeddings or None if failed
        """
        if not texts:
            return None
            
        self._ensure_model_loaded()
        
        for retry in range(self.max_retries):
            try:
                start_time = time.time()
                
                # Use smaller sub-batches for actual encoding
                sub_batch_size = min(8, len(texts))
                
                embeddings = self._model.encode(
                    texts,
                    batch_size=sub_batch_size,
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                
                elapsed = time.time() - start_time
                logger.debug(f"Encoded {len(texts)} texts in {elapsed:.2f}s")
                
                return embeddings.astype(np.float32, copy=False)
                
            except Exception as e:
                logger.warning(f"Batch encoding attempt {retry + 1} failed: {e}")
                if retry == self.max_retries - 1:
                    logger.error(f"Failed to encode batch after {self.max_retries} attempts")
                    return None
                    
                # Brief pause before retry
                time.sleep(1.0)
                gc.collect()
        
        return None
    
    def encode_texts_batched(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        Encode list of texts using safe batching
        
        Args:
            texts: List of text strings to encode
            
        Returns:
            Numpy array of all embeddings concatenated
        """
        if not texts:
            return None
            
        print(f"Generating embeddings for {len(texts)} texts in batches of {self.batch_size}...")
        
        all_embeddings = []
        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size
        
        for i in range(0, len(texts), self.batch_size):
            batch_num = (i // self.batch_size) + 1
            batch_texts = texts[i:i + self.batch_size]
            
            print(f"Processing batch {batch_num}/{total_batches} ({len(batch_texts)} texts)...")
            
            batch_embeddings = self.encode_batch_safe(batch_texts)
            
            if batch_embeddings is not None:
                all_embeddings.append(batch_embeddings)
                logger.debug(f"Batch {batch_num} successful: {batch_embeddings.shape}")
            else:
                logger.error(f"Batch {batch_num} failed completely")
                # Create fallback embeddings (zeros) to maintain data integrity
                fallback_dim = 384  # all-MiniLM-L6-v2 dimension
                fallback_embeddings = np.zeros((len(batch_texts), fallback_dim), dtype=np.float32)
                all_embeddings.append(fallback_embeddings)
                logger.warning(f"Using fallback zero embeddings for batch {batch_num}")
            
            # Memory cleanup and brief pause
            gc.collect()
            time.sleep(0.1)  # Prevent system overload
        
        if all_embeddings:
            final_embeddings = np.vstack(all_embeddings)
            print(f"Successfully generated embeddings: {final_embeddings.shape}")
            return final_embeddings
        else:
            logger.error("Failed to generate any embeddings")
            return None
    
    def __del__(self):
        """Cleanup on destruction"""
        self._cleanup_multiprocessing()


def create_safe_embedder() -> SafeEmbedder:
    """Factory function to create SafeEmbedder with standard settings"""
    return SafeEmbedder(
        model_name=os.getenv("RAG_EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2"),
        device=os.getenv("RAG_EMBED_DEVICE", "cpu"),
        batch_size=int(os.getenv("RAG_EMBED_BATCH_SIZE", "16")),
        max_retries=int(os.getenv("RAG_EMBED_MAX_RETRIES", "3"))
    )


# Test function
if __name__ == "__main__":
    print("Testing SafeEmbedder...")
    
    test_texts = [
        "The child completed the coloring task with steady movements.",
        "Movement velocity showed high variability during the session.",
        "Palm touches were frequent, indicating possible motor difficulties.",
        "Session completed within normal time range.",
        "Color selection patterns showed systematic approach."
    ]
    
    embedder = create_safe_embedder()
    embeddings = embedder.encode_texts_batched(test_texts)
    
    if embeddings is not None:
        print(f"✅ SafeEmbedder test successful! Shape: {embeddings.shape}")
    else:
        print("❌ SafeEmbedder test failed")
